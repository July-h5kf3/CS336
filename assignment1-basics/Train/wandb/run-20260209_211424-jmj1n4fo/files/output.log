WandB logging enabled.
Using device: cpu
Loading data from /home/acd66/project/CS336/assignment1-basics/tests/fixtures/tinystories_sample_5M.npy...
Data shape: (1319329,), dtype: uint16
Initializing model...
Total trainable parameters: 23,089,664

=== Training Config ===
Batch size: 32
Context length: 256
Total steps: 40000
LR: 1e-5 -> 1e-3 (warmup: 500)
Starting from step: 0
Training:   0%| | 0/40
Traceback (most recent call last):
  File "/home/acd66/project/CS336/assignment1-basics/Train/train.py", line 202, in <module>
    train(args.config, args.resume)
  File "/home/acd66/project/CS336/assignment1-basics/Train/train.py", line 141, in train
    logits = model.forward(xb)  # (batch, seq_len, vocab_size)
             ^^^^^^^^^^^^^^^^^
  File "/home/acd66/project/CS336/assignment1-basics/cs336_basics/Transformer.py", line 30, in forward
    x = block(x,token_positions)
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/acd66/project/CS336/assignment1-basics/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/acd66/project/CS336/assignment1-basics/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/acd66/project/CS336/assignment1-basics/cs336_basics/Transformer_Block.py", line 17, in forward
    attn_out = self.attention(x_norm,token_positions)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/acd66/project/CS336/assignment1-basics/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/acd66/project/CS336/assignment1-basics/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/acd66/project/CS336/assignment1-basics/cs336_basics/CausalMultiHeadSelfAttention.py", line 98, in forward
    Q = self.RoPE(Q,token_positions)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/acd66/project/CS336/assignment1-basics/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/acd66/project/CS336/assignment1-basics/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/acd66/project/CS336/assignment1-basics/cs336_basics/RotaryPositionalEmbedding.py", line 48, in forward
    x_rot_even = x_even * cos - x_odd * sin
                 ~~~~~~~^~~~~
RuntimeError: The size of tensor a (16) must match the size of tensor b (32) at non-singleton dimension 1
