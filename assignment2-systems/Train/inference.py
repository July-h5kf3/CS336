"""
Transformer è¯­è¨€æ¨¡åž‹æŽ¨ç†è„šæœ¬
æ‰€æœ‰å‚æ•°ä»Ž config.yaml è¯»å–ï¼Œå‘½ä»¤è¡Œä»…æŒ‡å®š --config å’Œ --prompt
"""
import os
import sys
import argparse
import yaml
import torch

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from cs336_basics.Transformer import Transformer
from cs336_basics.BPE_tokenizer import BPETokenizer
from cs336_basics.data_utils import load_checkpoint


def load_config(config_path: str) -> dict:
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def apply_top_p(logits, top_p):
    if top_p >= 1.0:
        return logits
    sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
    sorted_mask = cumulative_probs - torch.softmax(sorted_logits, dim=-1) > top_p
    sorted_logits[sorted_mask] = -float('inf')
    return torch.gather(sorted_logits, -1, torch.argsort(sorted_indices, dim=-1))


@torch.no_grad()
def generate(model, tokenizer, prompt, max_new_tokens, device, context_length,
             temperature=0.8, top_p=1.0, eos_token=None):
    model.eval()
    prompt_ids = tokenizer.encode(prompt)
    tokens = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)

    # èŽ·å– eos token id
    eos_id = None
    if eos_token is not None:
        try:
            eos_ids = tokenizer.encode(eos_token)
            if len(eos_ids) == 1:
                eos_id = eos_ids[0]
        except Exception:
            pass

    generated_ids = []
    for _ in range(max_new_tokens):
        if tokens.size(1) > context_length:
            tokens = tokens[:, -context_length:]

        logits = model(tokens)[:, -1, :]

        if temperature != 1.0 and temperature > 0:
            logits = logits / temperature
        
        # ä»…ä½¿ç”¨ top-p é‡‡æ ·
        if top_p < 1.0:
            logits = apply_top_p(logits, top_p)

        if temperature == 0:
            next_token = torch.argmax(logits, dim=-1, keepdim=True)
        else:
            next_token = torch.multinomial(torch.softmax(logits, dim=-1), num_samples=1)

        tokens = torch.cat((tokens, next_token), dim=-1)
        next_id = next_token.item()
        generated_ids.append(next_id)

        if eos_id is not None and next_id == eos_id:
            break

    return tokenizer.decode(prompt_ids + generated_ids)


def main():
    parser = argparse.ArgumentParser(description="Transformer è¯­è¨€æ¨¡åž‹æŽ¨ç†")
    parser.add_argument("--config", default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--prompt", type=str, required=True, help="è¾“å…¥æç¤ºæ–‡æœ¬")
    args = parser.parse_args()

    # â”€â”€â”€ åŠ è½½é…ç½® â”€â”€â”€
    config = load_config(args.config)
    model_config = config['model']
    preprocess_config = config.get('preprocess', {})
    train_config = config.get('training', {})
    infer_config = config.get('inference', {})

    device = train_config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ðŸ“Œ Device: {device}")

    # â”€â”€â”€ Tokenizer â”€â”€â”€
    special_tokens = preprocess_config.get('special_tokens')
    if isinstance(special_tokens, str):
        special_tokens = [special_tokens]
    tokenizer = BPETokenizer.from_files(
        preprocess_config['vocab'],
        preprocess_config['merges'],
        special_tokens
    )
    print(f"âœ“ Tokenizer (vocab={len(tokenizer.vocab)})")

    # â”€â”€â”€ æ¨¡åž‹ â”€â”€â”€
    context_length = int(model_config['context_length'])
    model = Transformer(
        vocab_size=int(model_config['vocab_size']),
        context_length=context_length,
        num_layers=int(model_config['num_layers']),
        d_model=int(model_config['d_model']),
        num_heads=int(model_config['num_heads']),
        device=device,
    )
    print(f"âœ“ æ¨¡åž‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # â”€â”€â”€ Checkpoint â”€â”€â”€
    checkpoint_path = infer_config.get('checkpoint')
    if checkpoint_path is None:
        checkpoint_dir = train_config.get('checkpoint_dir', 'checkpoints')
        checkpoint_path = os.path.join(checkpoint_dir, 'ckpt_final.pt')

    if os.path.exists(checkpoint_path):
        # ä½¿ç”¨ data_utils ä¸­çš„ load_checkpoint
        try:
            step = load_checkpoint(checkpoint_path, model, optimizer=None)
            print(f"âœ“ åŠ è½½ checkpoint (step={step})")
        except Exception as e:
            print(f"âš  åŠ è½½ checkpoint å¤±è´¥: {e}")
            print("  å°†ä½¿ç”¨éšæœºæƒé‡")
    else:
        print(f"âš  Checkpoint ä¸å­˜åœ¨: {checkpoint_path}ï¼ˆä½¿ç”¨éšæœºæƒé‡ï¼‰")

    # â”€â”€â”€ æŽ¨ç†å‚æ•°ï¼ˆä»… top-pï¼‰ â”€â”€â”€
    max_new_tokens = int(infer_config.get('max_new_tokens', 200))
    temperature = float(infer_config.get('temperature', 0.8))
    top_p = float(infer_config.get('top_p', 0.95))
    eos_token = infer_config.get('eos_token', None)

    # â”€â”€â”€ ç”Ÿæˆ â”€â”€â”€
    print(f"\nPrompt: {args.prompt}")
    print(f"Temperature={temperature}  Top-p={top_p}  Max tokens={max_new_tokens}")
    print("â”€" * 60)

    generated_text = generate(
        model, tokenizer, args.prompt,
        max_new_tokens=max_new_tokens,
        device=device,
        context_length=context_length,
        temperature=temperature,
        top_p=top_p,
        eos_token=eos_token,
    )

    print(generated_text)
    print("â”€" * 60)
    prompt_tokens = len(tokenizer.encode(args.prompt))
    total_tokens = len(tokenizer.encode(generated_text))
    print(f"Prompt: {prompt_tokens} tokens â†’ ç”Ÿæˆ: {total_tokens - prompt_tokens} tokens")


if __name__ == "__main__":
    main()