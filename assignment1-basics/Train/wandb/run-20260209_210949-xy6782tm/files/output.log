WandB logging enabled.
Using device: cpu
Loading data from /home/acd66/project/CS336/assignment1-basics/tests/fixtures/tinystories_sample_5M.npy...
Data shape: (1319329,), dtype: uint16
Initializing model...
Total trainable parameters: 19,157,504

=== Training Config ===
Batch size: 32
Context length: 256
Total steps: 40000
LR: 1e-5 -> 1e-3 (warmup: 500)
Starting from step: 0
Training:   0%|                                                                              | 0/40000 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/acd66/project/CS336/assignment1-basics/Train/train.py", line 202, in <module>
    train(args.config, args.resume)
  File "/home/acd66/project/CS336/assignment1-basics/Train/train.py", line 141, in train
    logits = model.forward(xb)  # (batch, seq_len, vocab_size)
             ^^^^^^^^^^^^^^^^^
  File "/home/acd66/project/CS336/assignment1-basics/cs336_basics/Transformer.py", line 27, in forward
    x = self.TokenEmbedding(token_ids)  # [batch_size, seq_len, d_model]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/acd66/project/CS336/assignment1-basics/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/acd66/project/CS336/assignment1-basics/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/acd66/project/CS336/assignment1-basics/cs336_basics/Embedding.py", line 19, in forward
    return self.weight[token_ids]
           ~~~~~~~~~~~^^^^^^^^^^^
IndexError: tensors used as indices must be long, int, byte or bool tensors
